%\documentclass[soln,12pt,draft]{utarexam}
 \documentclass[soln,12pt]{utarexam}
 %\documentclass[12pt]{utarexam}
 %\usepackage[margin=2cm,includefoot]{geometry}
 
 
 \usepackage{amsmath,amsthm, amssymb}
 %\usepackage{amsfonts}
 \usepackage{enumerate}
 \usepackage{color}
 \usepackage{verbatim}
 %\usepackage{multicol}
 \usepackage{lifecon}
 \usepackage{multirow}
 %\usepackage{framed}
 %\usepackage{showkeys}
 \usepackage{bm}
 
 %%% IMPORTANT
 \newcommand{\thesubject}{MEME16203Linear Models}
 \pagestyle{examheading}
 %%%
 
 % http://en.wikibooks.org/wiki/LaTeX/Colors
 \definecolor{mycolour}{rgb}{0,0,1.0}
 %\iflecturer
 %\definecolor{mycolour}{rgb}{0,0,1.0}
 %\else
 %\definecolor{mycolour}{rgb}{1,1,1} % White
 %\fi
 
 \newcommand \ds {\displaystyle}
 %\newcommand \lf {\left}
 %\newcommand \rg {\right
 %\input symbol
 
 \parindent=0pt
 \linespread{1.1}
 
 
 \begin{document}
 \underline{\textbf{Assignment 3}}\\
 \centerline{\textbf{UNIVERSITI TUNKU ABDUL RAHMAN}}
 
 \medskip
 %Name:\hspace{4cm}
 %\hfill
 %Student ID:\hspace{3cm}
 %\hfill
 %Mark:\hspace{2cm}/100
 %\bigskip
 
 \hrule
 
 \begin{tabbing}
 \hspace{2.8cm}\=\hspace{5cm}\=\hspace{2.8cm}\=\kill
   \textsc Faculty:    \> \textsc FES \>
   \textsc Unit Code:  \> \textsc MEME16203\\
   \textsc Course:     \> \textsc MAC \>
   \textsc Unit Title: \> \textsc Linear Models\\
   \textsc Year:       \> \textsc 1,2 \>
   %\textsc Lecturer:   \> Dr Yong Chin Khian \\
   \textsc Session:    \> May 2023\\
   \textsc Due by:     \> 28/7/2023
   \end{tabbing}
 \hrule
 
 
 
 \begin{enumerate}
% From: 
\item \label{NT-Q12b}
Consider the following models:
\begin{center}
\begin{tabular}{|c|c|}\hline
Model A:& Model B:\\
$\mathbf{Y}$ = $\mathbf{X}\bm{\beta} + \bm{\epsilon}$
= $\begin{bmatrix}
        1&-2&1\\1&-2&0\\1&-2&-1\\1&-1&1\\1&-1&0\\1&-1&-1\\1&1&1\\1&1&0\\1&1&-1
      \end{bmatrix}$$\begin{bmatrix} \beta_0 \\ \beta_1 \\ \beta_2\end{bmatrix} + \bm{\epsilon}$&$\mathbf{Y}$ = $\mathbf{W}\bm{\gamma} + \bm{\epsilon}$
= $\begin{bmatrix}
        1&1&0&0&1&0&0\\
        1&1&0&0&0&1&0\\
        1&1&0&0&0&0&1\\
        1&0&1&0&1&0&0\\
        1&0&1&0&0&1&0\\
        1&0&1&0&0&0&1\\
        1&0&0&1&1&0&0\\
        1&0&0&1&0&1&0\\
        1&0&0&1&0&0&1\\
      \end{bmatrix}$$\begin{bmatrix} \mu \\ \alpha_1 \\ \alpha_2 \\ \alpha_3 \\ \eta_1 \\ \eta_2 \\ \eta_3\end{bmatrix} + \bm{\epsilon}$.\\\hline
\end{tabular}
\end{center}
 where  $\bm{\epsilon} \sim N(0, \sigma^2\mathbf{I})$
  \begin{enumerate}
  \item Find matrix $\mathbf{G}$ such that $\mathbf{X} = \mathbf{WG}$. \hfill(10~marks)
    \begin{answer}~

           $\mathbf{X} = \mathbf{W}
      \begin{bmatrix}
        1&1&0\\0&-3&0\\0&-2&0\\0&0&0\\0&0&1\\0&0&0\\0&0&-1
      \end{bmatrix} = \mathbf{WG}$      
    \end{answer}
  
  \item What is the distribution of $\dfrac{\mathbf{y^T(I-P_X)P_W(I-P_X)y}}{\sigma^2}$? \hfill(10~marks)
    \begin{answer}~
      
In this case,\\
      $\mathbf{A} = \dfrac{\mathbf{(I-P_X)P_W(I-P_X)}}{\sigma^2}$ and $\bm{\Sigma} = \sigma^2\mathbf{I}$.\\ 
      $\mathbf{A}\bm{\Sigma} = \mathbf{(I-P_X)P_W(I-PX)} = \mathbf{(I-P_X)(P_W - P_WP_X)}$\\
      Note that since $\mathbf{X} = \mathbf{WF}$, then \\
      $\mathbf{P_WX} = \mathbf{P_WWF} = \mathbf{WF} = \mathbf{X}$ and hence\\ 
      $\mathbf{P_WP_X} = \mathbf{P_WX(X^TX)^{-1}X^T} = \mathbf{X(X^TX)^{-1}X^T} = \mathbf{P_X}$\\
      $\mathbf{P_XP_W} = \mathbf{P_X^TP_W^T} = \mathbf{(P_WP_X)^T} = \mathbf{P_X^T} = \mathbf{P_X}$
      \begin{tabbing}
        $\mathbf{A}\bm{\Sigma}$ \== $\mathbf{(I-P_X)P_W(I-PX)}$\\
        \>= $\mathbf{(I-P_X)(P_W - P_WP_X)}$\\
        \>= $\mathbf{(I-P_X)(P_W-P_X)}$\\
        \>= $\mathbf{(I-P_X)P_W} = \mathbf{P_W-P_X}$
      \end{tabbing}
      Thus,
      \begin{tabbing}
        $\mathbf{A}\bm{\Sigma}\mathbf{A}\bm{\Sigma}$ \== $\mathbf{P_W-P_X}\mathbf{P_W-P_X}$\\
        \>= $\mathbf{P_W^2}+\mathbf{P_X^2} - \mathbf{P_WP_X} - \mathbf{P_WP_X} = \mathbf{P_W} + \mathbf{P_X} - \mathbf{P_X} - \mathbf{P_X}$\\
        \>= $\mathbf{P_W} -\mathbf{P_X}$
      \end{tabbing}
 Thus, $\mathbf{A}\bm{\Sigma}$ is idempotent.\\
 Rank$(\mathbf{A})$ = Rank$(\mathbf{P_W}$-$\mathbf{P_X})$ = rank$(\mathbf{W})$ - Rank$(\mathbf{X})$ = 5-3 = 2\\
 $\lambda = \dfrac{\bm{\gamma}\mathbf{^TW^T(I-P_X)P_W(I-P_X)W}\bm{\gamma}}{2\sigma^2} = \dfrac{\bm{\gamma}\mathbf{^TW^T(P_W-P_X)W}\bm{\gamma}}{2\sigma^2} $\\
$\therefore \dfrac{\mathbf{y^T(I-P_X)P_W(I-P_X)y}}{\sigma^2} \sim \chi^2_1(\lambda)$      
    \end{answer}

  \item Show that $F = \dfrac{c\mathbf{y}^T(I-P_X)P_W(I-P_X)\mathbf{y}}{\mathbf{y}^T(I-P_W)\mathbf{y}}$ has an $F-$ distribution for some constant $c$ when model B is the correct model. Report a numerical value for $c$ and degrees of freedom. \hfill(10~marks)
    \begin{answer}~


      Let $\mathbf{W_2} = \dfrac{\mathbf{y^T(I-P_W)y}}{\sigma^2}$\\
      Here $\mathbf{A_2} = \dfrac{\mathbf{I-P_W}}{\sigma^2}$ and $\bm{\Sigma} = \sigma^2\mathbf{I}$, thus $A\Sigma = I-P_W$ is clearly idempotent. \\
$\mathbf{A_2}\bm{\mu} = \dfrac{\mathbf{I-P_W}}{\sigma^2}\mathbf{W}\bm{\gamma} = 0$ since $\mathbf{P_WW} = \mathbf{W}$. \\
D.F = rank($\mathbf{I-P_W}$) = 9-5=4\\ 
Thus $\mathbf{W_2} \sim \chi_4^2$\\ 
      Note that $\mathbf{W_1} = \dfrac{\mathbf{y^T(I-P_X)P_W(I-P_X)y}}{\sigma^2} \sim \chi^2_2(\lambda)$ and\\
 $\mathbf{W_2} = \frac{\mathbf{y^T(I-P_W)y}}{\sigma^2} \sim \chi^2_4$ and 
      \begin{tabbing}
        $\mathbf{(I-P_X)P_W(I-P_X)}(\sigma^2\mathbf{I})\mathbf{(I-P_W)}$\\
        \== $\sigma^2\mathbf{(P_W-P_X)(I-P_W)}$\\
        \>= $\sigma^2\mathbf{(P_W -P_W^2 -P_X -P_XP_W)}$\\
        \>= $\sigma^2\mathbf{(P_W-P_W-P_X-P_X)}$\\
        \>= $\mathbf{0}$
      \end{tabbing}
      Thus, $\mathbf{y^T(I-P_W)y}$ and $\mathbf{y^T(I-P_X)P_W(I-P_X)y}$ are independent random variables.\\
      Hence, $F = \dfrac{\mathbf{W_1}/2}{\mathbf{W_2}/4} = \frac{4\mathbf{W_1}}{2\mathbf{W_2}} \sim F_{2,4}(\lambda)$.\\
     $c = 2$. The degrees of freedom are $(2,4)$
      
    \end{answer}
\end{enumerate}



  
% From: Stat511S02Assignment6Q3
\item \label{NT-Q17}
Consider the model 
$$Y_i = \beta_0 + \beta_1X_i + \epsilon_i \hfill(1)$$
where $\epsilon \sim  NID(0, \sigma^2)$. This notation means that the random errors (and the
observations) have normal distributions and satisfy the Gauss-Markov property.\\
Define\\
$\mathbf{X} = \begin{bmatrix}1&X_1\\ 1&X_2 \\ \vdots&\vdots\\1&X_n\end{bmatrix}$ and $\mathbf{1} = \begin{bmatrix}1\\1\\ \vdots\\1\end{bmatrix}$ and $\mathbf{P_X} = \mathbf{X(X^TX)^{-1}X^T}$ and $\mathbf{P_1} = \mathbf{1(1^T1)^{-1}1^T}$ 
\begin{enumerate}
\item What are the distribution of the quadratic forms $\frac{1}{\sigma^2}\mathbf{Y^T(P_X-P_1)Y}$ and $\frac{1}{\sigma^2}\mathbf{Y^T(I-P_X)Y}$.
\hfill(10~marks)
\begin{answer}~

Note that under model 1, $\mathbf{Y} \sim N(\mathbf{X}\bm{\beta}, \sigma^2\mathbf{I})$ where $\bm{\beta} = \begin{bmatrix}\beta_0\\\beta_1\end{bmatrix}$\\
Let $\mathbf{A}_1 = \mathbf{P_1}$, $\mathbf{A}_2 = \mathbf{P_X-P_1}$ and $\mathbf{A}_3 = \mathbf{I-P_X}$. Then
\begin{itemize}
\item $\mathbf{A}_1, \mathbf{A}_2$ and $\mathbf{A}_3$ are all $n \times n$ symmetric matrices,
\item $\mathbf{A}_1 + \mathbf{A}_2 + \mathbf{A}_3 = \mathbf{I}$, and
\item rank($\mathbf{A}_1$) + rank($\mathbf{A}_2$) + rank($\mathbf{A}_3$) = $1 + (2-1) + (n-2)$ = $n$.
\end{itemize}
Then, by Cochran's Theorem, $\frac{1}{\sigma^2}\mathbf{Y^T\mathbf{A}_iY} \sim \chi^2_{r_i}(\frac{1}{\sigma^2}(\mathbf{X}\bm{\beta})^T\mathbf{A}_i\mathbf{X}\bm{\beta})$\\
Since rank($\mathbf{P_X-P_1}$) = 2-1 = 1, then \\
$\frac{1}{\sigma^2}\mathbf{Y^T(P_X-P_1)Y} \sim \chi^2_1(\frac{1}{\sigma^2}(\mathbf{X}\bm{\beta})^T(\mathbf{P_X-P_1})\mathbf{X}\bm{\beta})$\\
Since rank($\mathbf{I-P_X}$) = $n-2$, then \\
$\frac{1}{\sigma^2}\mathbf{Y^T(I-P_X)Y} \sim \chi^2_{n-2}(\frac{1}{\sigma^2}(\mathbf{X}\bm{\beta})^T(\mathbf{I-P_X})\mathbf{X}\bm{\beta}) \sim \chi_{n-2}^2$ since $(\mathbf{I-P_X})\mathbf{X}\bm{\beta}=0$\\
\end{answer}

\item Derive the distribution of $F = \frac{(n-2)\mathbf{Y^T(P_X-P_1)Y}}{\mathbf{Y^T(I-P_X)Y}}$.
Report degrees of freedom and a formula for the noncentrality parameter. \hfill(10~marks)
\begin{answer}~ 

By Cochran's Theorem and part (a), $\frac{1}{\sigma^2}\mathbf{Y^T(P_X-P_1)Y}$ and $\frac{1}{\sigma^2}\mathbf{Y^T(I-P_X)Y}$ are independent chi-square distributed with 1 and $n-2$ df respevtively. Then,
$$\frac{\frac{1}{\sigma^2}\mathbf{Y^T(P_X-P_1)Y}/2}{\frac{1}{\sigma^2}\mathbf{Y^T(I-P_X)Y}/(n-2)} =  \frac{(n-2)\mathbf{Y^T(P_X-P_1)Y}}{\mathbf{Y^T(I-P_X)Y}}
\sim F_{1, n-2}(\lambda)$$
where $\lambda = \frac{1}{\sigma^2}(\mathbf{X}\bm{\beta})^T(\mathbf{P_X-P_1})\mathbf{X}\bm{\beta}$  
\end{answer}

\item What is the null hypothesis associated with the $F$ statistic in part (b)? Justify your
answer by showing that the noncentrality parameter in part (b) is zero if and only if
the null hypothesis is true.\hfill(10~marks)
\begin{answer}~

\begin{tabbing}
$\lambda$ \== $\frac{1}{\sigma^2}(\mathbf{X}\bm{\beta})^T(\mathbf{P_X-P_1})\mathbf{X}\bm{\beta}$\\
\>= $\frac{1}{\sigma^2}[(\mathbf{X}\bm{\beta})^T(\mathbf{I-P_1})\mathbf{X}\bm{\beta}-(\mathbf{X}\bm{\beta})^T(\mathbf{I-P_X})\mathbf{X}\bm{\beta}]$\\
\>= $\frac{1}{\sigma^2}(\mathbf{X}\bm{\beta})^T(\mathbf{I-P_1})\mathbf{X}\bm{\beta}$ Since $\mathbf{I-P_X)X} = \mathbf{0}$\\
\>= $\frac{1}{\sigma^2}(\mathbf{X}\bm{\beta})^T(\mathbf{I-P_1})(\mathbf{I-P_1})\mathbf{X}\bm{\beta}$ since $(\mathbf{I-P_1})$ is idempotent\\
\>= $\frac{1}{\sigma^2}[(\mathbf{I-P_1})\mathbf{X}\bm{\beta}]^T(\mathbf{I-P_1})\mathbf{X}\bm{\beta}$
\end{tabbing}
Note that 
\begin{tabbing}
$(\mathbf{I-P_1})\mathbf{X}\bm{\beta}$ \== $\begin{bmatrix}(\mathbf{I-P_1})\mathbf{1}&(\mathbf{I-P_1}\mathbf{U})\end{bmatrix}\bm{\beta}$ where $\mathbf{U} = [X_1,\ldots, X_n]^T$\\
\>= $\begin{bmatrix}\mathbf{0}& \mathbf{U}- \mathbf{\bar U} \mathbf{1} \end{bmatrix}\bm{\beta}$ where $\mathbf{\bar U}$ = $\bar X$ = $\frac{\sum_{i=1}^nX_i}{n}$\\
\>= $\beta_1(\mathbf{U}- \mathbf{\bar U} \mathbf{1})$
\end{tabbing}
Thus
\begin{tabbing}
$\lambda$ \== $\frac{1}{\sigma^2}\beta_1^2(\mathbf{U}- \mathbf{\bar U} \mathbf{1})^T(\mathbf{U}- \mathbf{\bar U} \mathbf{1})$\\
\>= $\frac{\beta_1^2\sum_{i=1}^n(X_i-\bar X)^2)}{\sigma^2}$
\end{tabbing}
Thus, $\lambda = 0$ if and only if $\beta_1 = 0$. Therefore, the null hypothesis is $H_0: \beta_1 = 0$,
and the non-centrality parameter $\lambda$ is zero if and only if this null hypothesis is true.
\end{answer}

\item Suppose\\
$$\mathbf{Z} = \begin{bmatrix}1&X_1&X_1^2\\1&X_2&X_2^2\\\vdots\\1&X_n&X_n^2\\\end{bmatrix}.$$ 
Does $\frac{\mathbf{Y^T(P_X-P_1)Y}}{\mathbf{Y^T(I-P_Z)Y}/(n-3)}$, where $\mathbf{P_Z} = \mathbf{Z(Z^TZ)^{-1}Z}$,  have an F-dsitribution when $\mathbf{Y} \sim N(\mathbf{X}\bm{\beta}, \sigma^2\mathbf{I})$ is true? Explain. \hfill(10~marks)

\begin{answer}~

Let $\mathbf{A}_1 = \mathbf{P_1}$, $\mathbf{A}_2 = \mathbf{P_X-P_1}$, $\mathbf{A}_3 = \mathbf{P_Z-P_X}$ and $\mathbf{A}_4 = \mathbf{I-P_Z}$. Then
\begin{itemize}
\item $\mathbf{A}_1, \mathbf{A}_2, \mathbf{A}_3$ and $\mathbf{A}_4$ are all $n \times n$ symmetric matrices,
\item $\mathbf{A}_1 + \mathbf{A}_2 + \mathbf{A}_3+ \mathbf{A}_4 = \mathbf{I}$, and
\item rank($\mathbf{A}_1$) + rank($\mathbf{A}_2$) + rank($\mathbf{A}_3$)+ rank($\mathbf{A}_4$) = $1 + (2-1) + (3-2) +(n-3)$ = $n$.
\end{itemize}
Then, by Cochran's Theorem,
$\frac{1}{\sigma^2}\mathbf{Y^T(I-P_Z)Y} \sim \chi^2_{n-3}(\lambda_2)$where $\lambda_2 =\frac{1}{\sigma^2}(\mathbf{X}\bm{\beta})^T(\mathbf{I-P_Z})\mathbf{X}\bm{\beta}$, and $\mathbf{Y^TA_1Y}, \mathbf{Y^TA_2Y}, \mathbf{Y^TA_3Y}$ and $\mathbf{Y^A_4}$ are distributed independently.\\
$\frac{\mathbf{Y^T(P_X-P_1)Y}}{\mathbf{Y^T(I-P_Z)Y}/(n-3)}$  will have an F-distribution if $\mathbf{Y^T(P_X-P_1)Y}$ and $\mathbf{Y^T(I-P_Z)Y}$ are independent and $\lambda = 0$.  $\mathbf{Y^T(P_X-P_1)Y}$ and $\mathbf{Y^T(I-P_Z)Y}$ are independent by Cochran's theorem. Now
\begin{tabbing}
$\lambda_2$ \== $\frac{1}{\sigma^2}(\mathbf{X}\bm{\beta})^T(\mathbf{I-P_Z})\mathbf{X}\bm{\beta}$\\
\>= $\frac{1}{\sigma^2}(\mathbf{X}\bm{\beta})^T(\mathbf{I-P_Z})\mathbf{P_XX}\bm{\beta}$since $\mathbf{P_XX} = \mathbf{X}$\\
\>= $\frac{1}{\sigma^2}(\mathbf{X}\bm{\beta})^T(\mathbf{P_X-P_ZP_X})\mathbf{X}\bm{\beta}$\\
\>= $\frac{1}{\sigma^2}(\mathbf{X}\bm{\beta})^T(\mathbf{P_X-P_X})\mathbf{X}\bm{\beta}$ SInce $\mathbf{P_ZP_X = P_X}$\\
\>= 0
\end{tabbing}
Note that
$\mathbf{P_ZP_X} = \mathbf{P_ZX(X^TX)^{-1}X^T} = \mathbf{X(X^TX)^{-1}X^T} = \mathbf{P_X}$ since $\mathbf{X} \in \mathcal{C}\mathbf{Z}$ and hance $\mathbf{P_ZX} = \mathbf{X}$
\end{answer}
\end{enumerate}


  
% From: Stat511S02Q3
\item \label{NT-Q18c}
A food scientist performed the following experiment to study 
    the effects of combining two different fats and three different 
    surfactants on the specific volume of bread loaves.
    Four batches of dough were made for each of the six combinations of
    fat and surfactant.
    Ten loaves of bread were made from each batch of dough and the average 
    volume of the ten loaves was recorded for each batch. Unfortunately, 
    some of the yeast used to make some batches of dough was ineffective 
    and data from the loaves made from those batches had to be removed 
    from the analysis. Fortunately, all six combinations of the levels 
    of fat and surfactant were observed at least once. The data
    (average volume of 10 loaves) are shown below.

    \begin{center}
      \begin{tabular}{|c|c|c|c|} \hline
        &\multicolumn{3}{|c|}{Surfactant}\\ \hline
        &A&B&C \\ \hline
        \multirow{3}{*}{Fat 1}&6.7&7.1&5.5 \\
        &4.3&5.9&6.4 \\
        &5.7&5.6&5.8 \\ \hline
        \multirow{4}{*}{Fat 2}&5.9&5.6&6.4 \\
        &7.4&6.8&5.1 \\
        &7.1&6.9&6.2 \\
        &7.0&7.2&6.3 \\ \hline       
      \end{tabular}
    \end{center}
    Consider the model 
    $Y_{ijk} = \mu + \alpha_i + \beta_j + \gamma_{ij} + \epsilon_{ijk}$
    where $\epsilon_{ijk} \sim NID(0, \sigma^2)$ and $Y_{ijk}$
    denotes the average of the volumes of ten loaves of bread made from 
    the $k^{th}$ batch of dough using the $i^{th}$ fat and the
    $j^{th}$ surfactant.  $\alpha_i$ is
       associated with the $i$-th level of fat, $\beta_j$ is
       associated with the $j$-th level of surfactant, and $\gamma_{ij}$
       is an interaction parameter.
\begin{enumerate}[(a)]
       \item Define $SSE = \sum_{i=1}^2 \sum_{j=1}^3
         \sum_{k=1}^4(y_{ijk} - \bar y_{ij\bullet})^2$, where $\bar y_{ij\bullet}
         = \frac14 (y_{ij1} + y_{ij2}+ y_{ij3}+ y_{ij4})$. Show that $\frac{SSE}{\sigma^2}$ has
         a chi-squares distribution. States the degrees of fredom. 
\hfill(15~marks)
         \begin{answer}~

           Define $\mathbf{y} = \mathbf{X}\bm{\beta}+\bm{\epsilon}$ where\\
           $\mathbf{y} = [y_{111}, y_{112}, y_{113}, y_{114}, y_{121}, y_{122}, y_{123},y_{124}, y_{131}, y_{132}, y_{133},y_{134},  y_{211}, y_{212}, y_{213},y_{214}, y_{221}, y_{222}, y_{223}, y_{224}, y_{231}, y_{232}, y_{233}, y_{234}]^T$\\
           $\bm{\beta} = [\mu, \alpha_1, \alpha_2, \beta_1, \beta_2, \beta_3, \gamma_{11}, \gamma_{12},\gamma_{13},  \gamma_{21}, \gamma_{22}, \gamma_{13}]^T$ and $\mathbf{X}$ is the corresponding model matrix. Then\\
$\frac{SSE}{\sigma^2} = \mathbf{y^T}\frac{(\mathbf{I-P_X})}{\sigma^2}\mathbf{y}$ where $\mathbf{P_X} = \mathbf{X}(\mathbf{X^T}\mathbf{X})^{-}\mathbf{X^T}$.\\
Here $\mathbf{A} = {y^T}\frac{(\mathbf{I-P_X})}{\sigma^2}$ and $\bm{\Sigma} = \sigma^2\mathbf{I}$, then\\
$\mathbf{A}\bm{\Sigma} = \mathbf{I-P_X}$ clearly is idempotent.\\
$\mathbf{A}\bm{\mu} = \frac{1}{\sigma^2}(\mathbf{I-P_X})\mathbf{X}\bm{\beta} = \mathbf{0}$.\\
rank($\mathbf{A}$) = rank$(\mathbf{I-P_X}) = n -$rank$(\mathbf{X})$ = 24-6 = 18\\
$\therefore \frac{SSE}{\sigma^2} \sim \chi_{18}^2$.           
         \end{answer}


       \item Consider the estimator $$\hat C = \bar y_{1 \bullet  \bullet} -
         \bar y_{2 \bullet  \bullet},$$ where
         $$\bar y_{i \bullet  \bullet} = \frac18 \sum_{j=1}^2 \sum_{k=1}^4
         y_{ijk}.$$ 
         Show that $$F = \frac{m(\hat C)^2}{SSE}$$ has an
         F-distribution for some constant $m$. Report the value of $m$
         and the degrees of freedom for the F-distribution. \hfill(15~marks)
         \begin{answer}~

           Let $\mathbf{a^T} = \frac{1}{12} [1,1,1,1,1,1,1,1,1,1,1,1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,-1,]$, \\
we have 
           $\widehat{\mathbf{C}} = \mathbf{a^Ty}$ and 
           $\widehat{\mathbf{C}}^2 = \mathbf{y^T aa^Ty}$\\
           Take $\mathbf{A} = \frac{\mathbf{aa^Ta^Ta}}{\sigma^2} 
           = \frac{6\mathbf{a}\mathbf{a}^T}{\sigma^2}$ and 
           $\bm{\Sigma} = \sigma^2\mathbf{I}$. \\
           Then $\mathbf{A}\bm{\Sigma} = 6\mathbf{a}\mathbf{a^T}$.\\
           $\mathbf{A}\bm{\Sigma} \mathbf{A} \bm{\Sigma} 
           = (6\mathbf{a}\mathbf{a^T})(6\mathbf{a}\mathbf{a^T}) 
           = 36 \mathbf{a}\mathbf{a^T}\mathbf{a}\mathbf{a^T} 
           = 36 \mathbf{a} (\frac16) \mathbf{a^T} 
           = 6 \mathbf{a}\mathbf{a^T}$\\
           $\therefore \mathbf{A}\bm{\Sigma}$ is idempotent. \\
           $\mathbf{a^T}\bm{\mu} = \mathbf{a^T} \mathbf{X} \bm{\beta} 
           = \alpha_1 - \alpha_2 + \bar \gamma_{1 \bullet} + \gamma_{2 \bullet}$, thus\\
           $\lambda = \bm{\mu}^{\mathbf{T}} \mathbf{a}\mathbf{a^T} \bm{\mu} 
           = \frac{1}{\sigma^2}(\alpha_1 - \alpha_2 + \bar \gamma_{1\bullet } + \gamma_{2\bullet})^2$  \\
           Rank$(\mathbf{A})$ = Rank$(\mathbf{a}\mathbf{a^T}) = 1$\\
           $\therefore \frac{6(\widehat{\mathbf{C}})^2}{\sigma^2} 
           = \frac{6\mathbf{y^T}\mathbf{a}\mathbf{a^T}\mathbf{y}}{\sigma^2} 
           \sim \chi_1^2(\lambda)$\\
           To show that $\widehat{\mathbf{C}}^2$ is independent of SSE. 
           Let $\mathbf{A_1} = \mathbf{I-P_X}$, $\mathbf{A_2} = \mathbf{a}\mathbf{a^T}$.
           \begin{tabbing}
             $\mathbf{A_1}\bm{\Sigma} \mathbf{A_2}$ 
             \== $(\mathbf{I-P_X}) \sigma^2\mathbf{I}(\mathbf{a}\mathbf{a^T})$\\
             \>= $\sigma^2[\mathbf{a}\mathbf{a^T-P_X}\mathbf{a}\mathbf{a^T}]$\\
             \>= $\sigma^2[\mathbf{a}\mathbf{a^T}-\mathbf{a}\mathbf{a^T}]$ since $\mathbf{a} \in \mathcal{C} (\mathbf{X})$\\
\>= $\mathbf{0}$
\end{tabbing}
Thus $\widehat{\mathbf{C}}^2$ and SSE are independent. Consequently,
\begin{tabbing}
  $F$ \== $\frac{\frac{6(\widehat{\mathbf{C}})^2}{\sigma^2}/1}{\frac{SSE}{\sigma^2}/18}$\\
  \>= $\frac{108 (\widehat{\mathbf{C}})^2}{SSE} \sim F_{1,18}(\lambda)$
\end{tabbing}
where $\lambda =  \frac{1}{\sigma^2} (\alpha_1 - \alpha_2 + \bar \gamma_{1\bullet } + \gamma_{2\bullet})$     
         \end{answer}

       \end{enumerate}  


  
\end{enumerate}
\end{document}
